# Usage Instructions

## Installation

---

This repository is a fork of Ankit Dhall, Kunal Chelani, Vishnu Radhakrishnan, and KM Krishna's `lidar_camera_calibration` package (<https://github.com/ankitdhall/lidar_camera_calibration?tab=readme-ov-file>).

It contains a Dockerfile and updated configuration files for use with the ACT Lab @ Brown's Spot Robot.

Suggested workflow is to edit the configuration files on the local machine, then build and run the Docker container which will mount the local directory into the container, allowing you to run the nodes with the updated configuration files.

## Configuring the lidar_camera_calibration Package

This package provides two main functionalities:

1. Calibration between a single camera and a Velodyne or Hesai-Pandar40P LiDAR via the `find_transform` node.
2. Fusion of the pointclouds generated by the LiDAR and multiple cameras via `pointcloud_fusion`.

The workflow is as follows:

1. Calibrate the intrinsic parameters of a single camera, for example using [Kalibr](https://github.com/ethz-asl/kalibr)
2. Generate the configuration files for the `lidar_camera_calibration` package using the data for that camera.
3. Run the `find_transform` node to find the transformation between the camera and the LiDAR.
4. Repeat steps 1-3 for each camera that needs to be calibrated with the LiDAR.
5. Use the `pointcloud_fusion` node to fuse the pointclouds from the LiDAR and the cameras.

For each camera, you will need to edit the following `conf` files:

### `conf/config_file.txt`

---

This file is of the form:

```
image_width image_height
x- x+
y- y+
z- z+
cloud_intensity_threshold
number_of_markers
use_camera_info_topic?
fx 0 cx 0
0 fy cy 0
0 0 1 0

MAX_ITERS

initial_rot_x initial_rot_y initial_rot_z

lidar_type
```

For information on these parameters, see <https://github.com/ankitdhall/lidar_camera_calibration?tab=readme-ov-file>

Assuming an experiment setup similar to the one described in the lidar_camera_calibration repository, such that the boards are almost stationary and the camera and the LiDAR are fixed, few of the parameters should need to be changed from the defaults.

For first attempt at calibration, adjust only

* `number_of_markers`
* `initial_rot_x initial_rot_y initial_rot_z`, used to specify the initial orientation of the lidar with respect to the camera, in radians. The default values are for the case when both the lidar and the camera are both pointing forward.
* `lidar_type`. 0 for Velodyne; 1 for Hesai-Pandar40P.
* `fx fy cx cy`, camera intrinsic parameters.

### `conf/marker_coordinates.txt`

---

Ensure that the Aruco markers are fixed to the calibration boards such that when hung from a corner, the markers are on the left side of the board. Arrange the boards such that Aruco ids are in ascending order. Note that these *must* be the 5x5 Aruco markers.

The first line of the file should be the number of markers N, followed by 5*N lines

```
length (s1)
breadth (s2)
border_width_along_length (b1)
border_width_along_breadth (b2)
edge_length_of_ArUco_marker (e)
```

where all dimensions are given in centimeters.

![](https://github.com/ankitdhall/lidar_camera_calibration/raw/master/images/board_dim_label.jpg)

### `conf/lidar_camera_calibration.yaml`

---

```
lidar_camera_calibration:
camera_frame_topic: /frontNear/left/image_raw
camera_info_topic: /frontNear/left/camera_info
velodyne_topic: /velodyne_points
```

Contains name of camera and velodyne topics that the node will subscribe to.

### `conf/calibrated_file.ini`

---

This is file required only for the `aruco_mapping` package called by `lidar_camera_calibration`.

It is of the form:

```
[image]

width
x

height
y

[camera_name]

camera matrix
fx 0 cx
0 fy cy
0 0 1

distortion
k1 k2 t1 t2 k3

rectification
1.00000 0.00000 0.00000
0.00000 1.00000 0.00000
0.00000 0.00000 1.00000

projection
fx 0 cx 0
0 fy cy 0
0 0 1 0 
```

Where `x, y` are the width and height of the camera image, `camera_name` helps identify this configuration,`fx, fy, cx, cy` are the intrinsic parameters of the camera (same as in `config_file.txt`), and `k1, k2, t1, t2, k3` are the distortion coefficients of the camera (also produced by the camera intrinsic calibration process).

### `launch/find_transform.launch`

---

In this launch file, you should only need to change the parameters `num_of_markers` and `marker_size` (edge length of arcuo marker in meters; measurement (e) from `conf/marker_coordinates.txt` converted to meters).

## Building and Running the Docker Container

In a terminal,

1. `cd /path/to/lidar_calibration_docker/`
2. `docker build -t lidar_calibration .`
3. (Optional) If you have problems running a GUI in the Docker, run `xhost +local:root` on the host machine to allow local docker containers to access the X server.
4.

```bash
docker run -v $(pwd)'data:/catkin_ws/data' -v /var/run/dbus/system_bus_socket:/var/run/dbus/system_bus_socket -v /dev:/dev --network=host -e "DISPLAY=unix$DISPLAY" -e "QT_X11_NO_MITSHM=1" -v "/tmp/.X11-unix:/tmp/.X11-unix:rw" -it lidar_calibration
```

5. `source devel/setup.sh`

## Usage

Before running the package, ensure that the ArUco markers are visible in the camera frame and that the markers are arranged in ascending order of their ids from left to right as viewed by the camera.

Run with
`roslaunch lidar_camera_calibration find_transform.launch`

The package will estimate an initial transformation matrix and then prompt the user to manually mark corners of each of the rectangular board. To select a corner, click on the point and press any key. Once 4 points are clicked, each followed by a key-press, the program will move on to the next line segment. Continue marking the line segments for all boards until complete. Line segments for each board are to be marked in clock-wise order starting from the top-left.

After marking all the line-segments, the rigid-body transformation between the camera and the LiDAR frame will be displayed.

## Recording Bags on Spot

1. ssh into the Core I/O Payload
2. clone `https://github.com/USC-ACTLab/spot_ros2/tree/milan`
3. Follow the instructions in `Core IO Deploy.md` below to build and run the Docker container with the Spot and Velodyne drivers.
4. `cd /ros_ws/data && ros2 bag record -o <bag_name> <topic1> <topic2> ...`
5. To stop recording, `Ctrl+C`
6. exit the docker container and `exit` the ssh session
7. on your local machine, `scp -P 20022 -r spot@ROBOT_IP_ADDRESS:~/data/<bag_name> /path/to/lidar_calibration_docker/data/`
8. `cd /path/to/lidar_calibration_docker/`
9. Install `rosbags` via `pip install rosbags` (might need to source a venv)
10. `rosbags-convert --src <bag-name>/ --dst <bag-name>.bag`
11. In `ros1_bag_camerainfo.py`, change the last line to `downgrade_camerainfo_to_rosbag1(Path('data/<bag_name>.bag'), Path('data/<bag_name>_downgraded.bag'))` to account for the different CameraInfo message structure in ros1 vs ros2.
12. Run the script: `python3 data/ros1_bag_camerainfo.py`
13. `cd .. && docker build -t lidar_docker .`
14. `docker run -v '/path/to/lidar_calibration/data:/catkin_ws/data' -v /var/run/dbus/system_bus_socket:/var/run/dbus/system_bus_socket -v /dev:/dev --network=host -e "DISPLAY=unix$DISPLAY" -e "QT_X11_NO_MITSHM=1" -v "/tmp/.X11-unix:/tmp/.X11-unix:rw" -it lidar_docker bash`
15. `source devel/setup.sh`
16. `rosbag play -l /catkin_ws/data/<bag_name>_downgraded --clock`
